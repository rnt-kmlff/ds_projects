Chapter 3: Linear Regression
lm(Y ~ var1, data)
lm(Y ~ var1 + var2 + I(var2^2) + …, data)
lm(Y ~ poly(var, n), data) - nth-order polynomial fit
summary(model)
confint(model) - confidence intervals for model’s coefficients
predict(model, data)
residuals(model) - residual of the model
rstudent(model) - studentized residuals
hatvalues(model) - leverage statistics
vif(model) - variance inflation factors (car package)

Chapter 4: Classification
glm(Y ~ var1 + var2 + …, data, family = binomial for logistic regression, or = poisson for Poisson regression)
predict(model, data = test data, type = “response” for output probabilities P(Y = 1 | X) )
table(model_predicted, Y) - confusion matrix
lda(Y ~ var, data) - LDA
qda(Y ~ var, data) - QDA
naiveBayes(Y ~ var, data) - naive Bayes model (e1071 package), by default uses a Gaussian distribution
knn(train.X - matrix of x from training data, test.X - matrix of x from test data, train.Y - vector of Y from train data, k = …) - KNN (class package)

Chapter 5: Resampling Methods
lm(Y ~ var, data, subset = train subset of data); predict(Y ~ var, data, subset = test) - subset selection

cv.glm(data, model) - LOOCV estimate
cv.error <- c() #vector of n CV errors
for (i in 1:n) {
glm.fit <- glm(Y ~ poly(var, i), data)
cv.error[I] <- cv.glm(data, glm.fit)$delta[1]
}

cv.error <- c() #vector of n k-fold CV errors
for (i in 1:n) {
glm.fit <- glm(Y ~ poly(var, i), data)
cv.error[I] <- cv.glm(data, glm.fit, K = k folds)$delta[1]
}

boot package
boot(data, statistic = function computing the statistic of interest, R = number of bootstrap replicates)

Chapter 6: Linear Models and Regularization Methods
regsubsets(Y ~ var, data, nvmax = number of predictors) - best subset selection for given number of predictors
regsubsets(Y ~ var, data, nvmax = number of predictors, method = “forward” or “backward”) - forward or backward stepwise selection

glmnet(x, y, alpha = 0 for ridge regression or = 1 for lasso fit, lambda = … by default all values, standardize = TRUE by default), where x is matrix of predictors, and y is a vector of response

pcr(Y ~ var, data, scale = TRUE for standardized predictors, validation = “CV” for ten-fold CV for each number of principal components) - principal components regression

plsr(rY ~ var, data, scale = TRUE for standardized predictors, validation = “CV” for ten-fold CV for each number of principal components) - partial least squares

Chapter 7: Moving beyond linearity
anova(model1, model2, etc) - analysis of variance of different nested models (predictors in model1 are subset of predictors in model2 etc)
lm(Y ~ cut(var, k)) - step function

splines package:
bs(var, knots = …, df = 3 by default) - generating a matrix of basis functions, by default cubic splines
lm(Y ~ bs(…))
ns(var, df = …) - natural spline
smooth.spline(Y, var, df = … or cv = TRUE for cross-validation of lambda) - smoothing spline
loess(Y ~ var, span = …) - local regression with span …
lm(Y ~ ns(var1, df) + ns(var2, df) +… ) - GAM with natural splines

gam package:
gam(Y ~ s(var1, df) + s(var, df) + …) - GAM with smoothing splines
lo(var, span = …) - local regression, can be used inside gam()
gam(I(Y > …) ~ var1 + s(var2, df) + …, family = binomial) - logistic regression GAM for Y > …

Chapter 8. Decision Trees
tree library
tree(Y ~ var, data = train dataset)
cv.tree(model, FUN = prune.misclass for classification, or deviance for regression) - CV for optimal level of tree complexity, result in number of CV errors ($dev), corresponding numbers of terminal nodes ($size) and cost-complexity parameter alpha ($k)
prune.misclass(model, best = … number of nodes with min $dev)
predict(model, data = test dataset, type = “class” for classification)

Bagging and Random Forests:
randomForest package
randomForest(Y ~ var, data = train dataset, mtry = p number of predictors for bagging, p/3 or sqrt(p) for regression and classification random forests, importance = TRUE, ntree = … number of trees)
predict(model, newdata = test dataset)
importance(model) - measures of each variable importance for the model

Boosting:
gbm package
gbm(Y ~ var, data = train data, distribution = “gaussian” for regression, “bernoulli” for classification, n.trees = …, interaction.depth = …, shrinkage = 0.001 by default, verbose = F)
summary(model) - relative influence plot
predict(model, newdata = test data, n.trees = …)

BART:
BART package
gbart() - for quantitative outcome variables, lbart() and pbart() - for binary outcomes
creating matrices of predictors for train and test datasets xtrain, xtest, ytrain, ytest:
gbart(xtrain, ytrain, x.test = xtest)

Chapter 9: Support Vector Machines
e1071, or LiblineaR packages

Support vector classifier:
data <- data.frame(x, y = as.factor(y)) - encoding response as factor variable
svm(y ~ ., data, kernel = “linear” for linear decision boundary, cost = … cost of violation to the margin, scale = FALSE to not scale each feature to have mean = 0 or sd = 1)
tune(svm, y ~ ., data, kernel = “linear”, ranges = list(cost = c(…))) - ten-fold CV of cost parameter for SVM; best model is stored at …$best.model
predict(bestmodel, test data)

SVM:
svm(y ~ ., data, kernel = “polynomial”, degree = …, or kernel = “radial”, gamma = …, cost = …, scale = FALSE)
tune(svm, y ~ ., data, kernel = “radial”, ranges = list(cost = c(…), gamma = c(…))) - CV for SVM with radial kernel to select best gamma and cost

ROC curves:
ROCR package
rocplot()

SVM with multiple classes:
svm() with y as a factor with more than two levels for multi-class classification using one-versus-one approach, or with y as a vector for support vector regression
